# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10czZJfby2Z4AgZrCRdOu6xVcV3B6DqhO

#1. download IMDB dataset
"""

#!pip install datasets

from datasets import load_dataset
imdb = load_dataset('imdb')
train_data = imdb['train']
test_data = imdb['test']

"""#2. Make vocab Dictionary and sentencepiece model"""

#!pip install sentencepiece

"""
I chose SentencePiece as the tokenizer because it is a subword-based tokenizer.
This significantly reduces the occurrence of the <unk> token.
"""

import sentencepiece as spm
with open('/content/drive/MyDrive/github/imdb-sentiment-comparison-rnn-transformer/src/sentencepiece/imdb.txt', 'w', encoding='utf-8') as f:
  for item in train_data:
    f.write(item['text'] + '\n')

"""
The reason I chose 20,000 as the vocabulary size is that the IMDb dataset has long reviews and a wide variety of expressions.
"""
spm.SentencePieceTrainer.train(
    input = '/content/drive/MyDrive/github/imdb-sentiment-comparison-rnn-transformer/src/sentencepiece/imdb.txt',
    model_prefix = '/content/drive/MyDrive/github/imdb-sentiment-comparison-rnn-transformer/src/sentencepiece/imdb',
    vocab_size = 20000,
    unk_id=0,
    pad_id=1,
    bos_id=2,
    eos_id=3
)

sp = spm.SentencePieceProcessor()
sp.load('/content/drive/MyDrive/github/imdb-sentiment-comparison-rnn-transformer/src/sentencepiece/imdb.model')

"""#3. Make Dataset & DataLoader"""

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class imdbDataset(Dataset):
    def __init__(self, data, max_len=512):
        self.data = data
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        encoded = sp.encode(self.data[idx]['text'])[:self.max_len]

        text = torch.tensor(encoded, dtype=torch.long)
        label = torch.tensor(self.data[idx]['label'], dtype=torch.float)
        return text, label

def collate_fn_rnn(batch):
  texts, labels = zip(*batch)
  lengths = torch.tensor([len(text) for text in texts], dtype=torch.long)
  padded_texts = pad_sequence(texts, batch_first=True, padding_value=sp.pad_id())
  labels = torch.stack(labels)
  return padded_texts, lengths, labels

def collate_fn_tr(batch):
    texts, labels = zip(*batch)
    padded_texts = pad_sequence(texts, batch_first=True, padding_value=sp.pad_id())
    mask = (padded_texts == sp.pad_id()).unsqueeze(1).unsqueeze(2)
    return padded_texts, mask, torch.stack(labels)

train_loader_rnn = DataLoader(imdbDataset(train_data), batch_size=64, shuffle=True, collate_fn=collate_fn_rnn)
train_loader_tr = DataLoader(imdbDataset(train_data), batch_size=8, shuffle=True, collate_fn=collate_fn_tr) #64 -> 8
test_loader_rnn = DataLoader(imdbDataset(test_data), batch_size=32, collate_fn=collate_fn_rnn)
test_loader_tr = DataLoader(imdbDataset(test_data), batch_size=8, collate_fn=collate_fn_tr) #64 -> 8