{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtbbpoUTzAqIYDcorj4Mgy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. RNN for baseline model"],"metadata":{"id":"BK7txpVLMrrG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"URGCgrBLFy8T"},"outputs":[],"source":["from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class IMDBRNN(nn.Module):\n","  def __init__(self, input_size, embed_dim, hidden_size, padding_idx):\n","    super().__init__()\n","    self.embedding = nn.Embedding(input_size, embed_dim, padding_idx=padding_idx)\n","    self.rnn = nn.RNN(embed_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n","    self.linear = nn.Linear(hidden_size, 1)\n","\n","  def forward(self, x, lengths):\n","    x = self.embedding(x) #(B, T, E)\n","    packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False) #shuffle=True in train_set\n","    out, h = self.rnn(packed) # dimension of h => (1, B, H)\n","    h_n = h[-1] # h_n -> (B, H)\n","    logits = self.linear(h_n).squeeze(-1) #(B, 1) -(squeeze(-1))-> (B,)\n","\n","    return logits"]},{"cell_type":"code","source":[],"metadata":{"id":"CGP56t6hMx_c"},"execution_count":null,"outputs":[]}]}