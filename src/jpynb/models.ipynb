{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTVjrGDu4TNMnAplqEgmvr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#1. RNN for baseline model"],"metadata":{"id":"GHA5TqiIdtTO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwauvCdudray"},"outputs":[],"source":["from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","class IMDBRNN(nn.Module):\n","  def __init__(self, input_size, embed_dim, hidden_size, padding_idx):\n","    super().__init__()\n","    self.embedding = nn.Embedding(input_size, embed_dim, padding_idx=padding_idx)\n","    self.rnn = nn.RNN(embed_dim, hidden_size=hidden_size, num_layers=1, batch_first=True)\n","    self.linear = nn.Linear(hidden_size, 1)\n","\n","  def forward(self, x, lengths):\n","    x = self.embedding(x) #(B, T, E)\n","    packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False) #shuffle=True in train_set\n","    out, h = self.rnn(packed) # dimension of h => (1, B, H)\n","    h_n = h[-1] # h_n -> (B, H)\n","    logits = self.linear(h_n).squeeze(-1) #(B, 1) -(squeeze(-1))-> (B,)\n","\n","    return logits"]},{"cell_type":"markdown","source":["#2. LSTM model"],"metadata":{"id":"dPqdOt5FfgNf"}},{"cell_type":"code","source":["class IMDBLSTM(nn.Module):\n","  def __init__(self, input_size, embed_dim, hidden_size, padding_idx):\n","    super().__init__()\n","    self.embedding = nn.Embedding(input_size, embed_dim, padding_idx=padding_idx)\n","    self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers=1, batch_first=True)\n","    self.linear = nn.Linear(hidden_size, 1)\n","\n","  def forward(self, x, lengths):\n","    x = self.embedding(x)\n","    packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n","    out, (h, c) = self.lstm(packed)\n","    h_n = h[-1]\n","    logits = self.linear(h_n).squeeze(-1)\n","\n","    return logits"],"metadata":{"id":"AuAR3INgfh53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. Transformer model"],"metadata":{"id":"B9ZxjHum6Uqb"}},{"cell_type":"markdown","source":["Positional Encoding"],"metadata":{"id":"fj6ugJkZfKX9"}},{"cell_type":"code","source":["import torch\n","import math\n","\n","class PositionalEncoding(nn.Module):\n","  def __init__(self, d_model, max_len=5000):\n","    super().__init__()\n","    pe = torch.zeros(max_len, d_model)\n","    position = torch.arange(0, max_len).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n","    pe[:, 0::2] = torch.sin(position * div_term)\n","    pe[:, 1::2] = torch.cos(position * div_term)\n","    self.register_buffer('pe', pe.unsqueeze(0))\n","\n","  def forward(self, x):\n","    return x + self.pe[:, :x.size(1)]"],"metadata":{"id":"djjF3V20fkV-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Multi-Head Attention"],"metadata":{"id":"L7kFdW4gfV49"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model, num_heads):\n","    super().__init__()\n","    self.num_heads = num_heads\n","    self.d_k = d_model // num_heads\n","\n","    self.w_q = nn.Linear(d_model, d_model)\n","    self.w_k = nn.Linear(d_model, d_model)\n","    self.w_v = nn.Linear(d_model, d_model)\n","    self.fc = nn.Linear(d_model, d_model)\n","\n","  def forward(self, q, k, v, mask=None):\n","    batch_size = q.size(0)\n","\n","    q = self.w_q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #(batch, seq, d_model) => (batch, heads, seq, d_k)\n","    k = self.w_k(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n","    v = self.w_v(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2)\n","\n","    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k) #(batch, heads, seq, seq)\n","    if mask is not None:\n","      scores = scores.masked_fill(mask == 1, -1e9)\n","\n","    attn = torch.softmax(scores, dim=-1)\n","    context = torch.matmul(attn, v) # [batch, heads, seq, d_k]\n","\n","    context  = context.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n","    return self.fc(context)"],"metadata":{"id":"0DEAyeDefEfE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Encoder Block"],"metadata":{"id":"eIhUc3mXmWTc"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","  def __init__(self, d_model, num_heads, d_ff):\n","    super().__init__()\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = nn.Sequential(\n","        nn.Linear(d_model, d_ff),\n","        nn.ReLU(),\n","        nn.Linear(d_ff, d_model)\n","    )\n","    self.layernorm1 = nn.LayerNorm(d_model)\n","    self.layernorm2 = nn.LayerNorm(d_model)\n","\n","  def forward(self, x, mask):\n","    attn_out = self.mha(x, x, x, mask)\n","    x = self.layernorm1(x + attn_out) #Add & Norm\n","\n","    ffn_out = self.ffn(x)\n","    x = self.layernorm2(x + ffn_out)\n","    return x"],"metadata":{"id":"XWg1SKeBl-IP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformer Classifier"],"metadata":{"id":"92Dm8WIXnry-"}},{"cell_type":"code","source":["class TransformerClassifier(nn.Module):\n","  def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, num_classes, max_len):\n","      super().__init__()\n","      self.embedding = nn.Embedding(vocab_size, d_model)\n","      self.pos_encoding = PositionalEncoding(d_model, max_len)\n","      self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n","      self.classifier = nn.Linear(d_model, num_classes)\n","\n","  def forward(self, x, mask):\n","    x = self.embedding(x)\n","    x = self.pos_encoding(x)\n","\n","    for layer in self.layers:\n","      x = layer(x, mask)\n","\n","    x = x.mean(dim=1)\n","    return self.classifier(x)"],"metadata":{"id":"EpvmplC5ntj1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wvXlLd3rxuOO"},"execution_count":null,"outputs":[]}]}